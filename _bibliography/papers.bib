---
---


@unpublished{sun2025structured,
  title={Structured Context Selection for Efficient Retrieval-Augmented Generation},
  author = {Jiashuo Sun{*} and Yixuan Xie{*} and Jimeng Shi and Jiawei Han},
  abbr={WorkingPaper},
  year={2025},
  selected={true},
}

@unpublished{zequn2025efficient,
  title={Progressive Structure Extraction:From Explicit to Implicit },
  author={Xueqiang Xu{*} and Yixuan Xie{*} and Runchu Tian and Jiawei Han},
  abbr={WorkingPaper},
  year={2025},
  selected={true},
}

@unpublished{zequn2025efficient,
  title={Efficient Reasoning, Factual Certainty: A Dynamic Interaction between Generalist and Specialist Models for Medical Reasoning},
  author={Zequn Zhang and Yixuan Xie and Qian Niu},
  abbr={WorkingPaper},
  year={2025},
  selected={true},
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need (TemplateExample)},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017},
  abbr={NeurIPS},
  bibtex_show={true},
  selected={false},
  abstract={The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.},
}
